# PubMedQA PEFT â€“ config.yaml

# Project general features
project:
  name: pubmedqa_peft
  seed: 42
  output_dir: outputs

# Data
data:
  # paths to stratified csvs, should contain only 4 columns
  train_csv: data/clean_final/pubmedqa_train_clean.csv
  val_csv: data/clean_final/pubmedqa_val_clean.csv
  test_csv: data/clean_final/pubmedqa_test_clean.csv

  # input formatting
  max_input_length: 256          # truncate context to fit
  max_target_length: 4           # "yes"/"no"
  text_template: |
    Question: {question}
    Abstract: {contexts}
    Instruction: Answer "yes" or "no" based on the abstract.

# Model
model:
  backbone: google/flan-t5-xl       
  tokenizer: google/flan-t5-xl      

# Core
core:                              
  compare_base: true               # Base (no tuning)
  methods:                         # PEFT methods to train/eval
    - prompt_tuning
    - prefix_tuning

# PEFT
peft:
  # Prompt Tuning (soft tokens prepended once at input)
  prompt_num_virtual_tokens: 50         # CHANGED: 80, 50, 20

  # Prefix Tuning (prefix key/values injected at each attention module)
  prefix_num_virtual_tokens: 50         # CHANGED, 5

# Training
train:
  # specs
  epochs: 13
  batch_size: 2                         # CHANGED, was 8
  optimizer: adamw_torch                # CHANGED, was adafactor
  weight_decay: 0.0
  scheduler: constant_with_warmup
  warmup_ratio: 0.1

  # separate lrs for PEFT types (small adapters can use higher lr)
  lr_prompt: 7e-3                       # CHANGED, was 1e-3
  lr_prefix: 5e-4

  # checkpointing / logging
  save_strategy: epoch
  logging_steps: 10

# Device
compute:
  device: cuda                          # CHANGED, was MPS when I was on a MacOS
  mixed_precision: none      
  
# Evals
evaluation:
  stats:
    project_out: outputs
    splits: ["test"]
    tuned_methods: ["prompt_tuning", "prefix_tuning"]
    labels: ["no", "yes"]
    output_csv: outputs/stats_eval_test.csv
    bootstrap:
      B: 5000
      seed: 0
